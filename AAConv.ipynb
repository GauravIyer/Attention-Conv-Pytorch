{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AAConv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyWJU3FPznLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P-94NxL1opM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AAConv(nn.Module):\n",
        "  def __init__(self,ip_dim,op_dim,dk,dv,dq,num_heads,ker_size,height,width):\n",
        "    super(AAConv,self).__init__()\n",
        "    self.ip_dim=ip_dim\n",
        "    self.op_dim=op_dim\n",
        "    self.dk=dk\n",
        "    self.dv=dv\n",
        "    self.dq=dq\n",
        "    self.num_heads=num_heads\n",
        "    self.ker_size=ker_size\n",
        "    self.height=height\n",
        "    self.width=width\n",
        "    self.dk_per_head=self.dk//self.num_heads # We assume num_heads divides dk\n",
        "    self.dv_per_head=self.dv//self.num_heads\n",
        "    self.dq_per_head=self.dq//self.num_heads\n",
        "    self.rel_embeddings_w=nn.Parameter(1/(self.dk_per_head**0.5)+torch.rand(2*width-1,self.dk_per_head),requires_grad=True)\n",
        "    self.rel_embeddings_h=nn.Parameter(1/(self.dk_per_head**0.5)+torch.rand(2*height-1,self.dk_per_head),requires_grad=True)\n",
        "    self.conv_qkv=nn.Conv2d(inp_dim,dk+dv+dq,1)\n",
        "    self.softmax=nn.Softmax(dim=-1)\n",
        "    self.attention_conv=nn.Conv2d(dv,dv,1)\n",
        "    self.conv=nn.Conv2d(ip_dim,op_dim-dv,ker_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    qkv=self.lin_qkv(x)\n",
        "    q,k,v=torch.split(qkv,[self.dq,self.dk,self.dv],dim=1)\n",
        "    batch_size,_,H,W=q.size()\n",
        "    q=q.view([batch_size,self.num_heads,dq_per_head,H*W])\n",
        "    k=k.view([batch_size,self.num_heads,dk_per_head,H*W])\n",
        "    v=v.view([batch_size,self.num_heads,dv_per_head,H*W])\n",
        "    q=q/(self.dk_per_head**0.5)\n",
        "    qktrans=torch.einsum('ijkl,ijkm -> ijlm',q,k)\n",
        "    s_h,s_w=self.relative_pos_emeddings(q)\n",
        "    weights=self.softmax(qktrans+s_h+s_w)\n",
        "    attn=torch.einsum('ijkl,ijfl -> ijfk',weights,v)\n",
        "    attn=attn.contiguous().view(batch_size,self.dv,H,W)\n",
        "    attn_out=self.attention_conv(attn)\n",
        "    conv_out=self.conv(x)\n",
        "    op=torch.cat(conv_out,attn_out)\n",
        "    return op\n",
        "  \n",
        "  def relative_pos_embeddings(self,q):\n",
        "    bsz,num_heads,dkh,hw=q.size()\n",
        "    q=q.view(bsz,num_heads,dkh,self.height,self.width)\n",
        "    s_w=self.rel_1d(q,self.rel_emeddings_w,self.height,self.width,num_heads,[0,1,2,4,3,5])\n",
        "    s_h=self.rel_1d(q.premute(0,1,2,4,3),self.rel_emeddings_h,self.height,num_heads,[0,1,4,2,5,3])\n",
        "    return s_h,s_w\n",
        "\n",
        "  def rel_1d(self,q,rel_k,h,w,num_heads,trans_mask):\n",
        "    z=torch.einsum('bhdxy, md -> bhxym',q,rel_k).view([-1,num_heads*h,w,2*w-1])\n",
        "    z=self.rel_to_abs(z)\n",
        "    z=z.view([-1,num_heads,h*w,h*w])\n",
        "    return z\n",
        "\n",
        "  def rel_to_abs(self,x):\n",
        "    bsz,num_heads,l,_=x.size()\n",
        "    x=F.pad(x,(0,1),'constant',0)\n",
        "    flat_x=x.view([bsz,num_heads,l*(2*l)])\n",
        "    flat_x_padded=F.pad(flat_x,(0,l-1),'constant',0)\n",
        "    x=flat_x_padded.view([bsz,num_heads,l+1,2*l-1])\n",
        "    x=final_x[:,:,:l,l-1:]\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nZpBEB0ALcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}